{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb6eba94",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"A3.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f06207",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-68f2f0ed9883b594",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 3\n",
    "\n",
    "\n",
    "## **Due: April 29th (Saturday), 2023, 8:00pm (PT)**\n",
    "\n",
    "### **Instructions:**\n",
    "\n",
    "Your Jupyter notebook assignment will often have 3 elements: written answers, code answers, and quiz answers. For written answers, you may insert images of your handwritten work in code cells, or write your answers in markdown and LaTeX. For quiz answers, your `record.txt` file will record your answer choices in the quiz modules for submission. Both your quiz answers and code answers will be autograded on Gradescope. This assignment does not have the quiz portion.\n",
    "\n",
    "For all elements, DO NOT MODIFY THE CELLS. Put your answers **only** in the answer cells given, and **do not delete cells**. If you fail to follow these instructions, you will lose points on your submission.\n",
    "\n",
    "Make sure to show the steps of your solution for every question to receive credit, not just the final answer. You may search information online but you will need to write code/find solutions to answer the questions yourself. You will submit your .ipynb file and record.txt to gradescope when you are finished.\n",
    "\n",
    "### **Late Policy:**\n",
    "\n",
    "Late assignments will be accepted at 75% credit up to one week late. Consult the syllabus for more info on the late policy.\n",
    "\n",
    "### How to Include Your Math Written Answer?\n",
    "\n",
    "You could use inline $\\LaTeX$ in markdown (recommended) or use markdowns' include image functionality to submit your written responses.\n",
    "\n",
    "#### $\\LaTeX$ (recommended)\n",
    "[Here is a fantastic tutorial from CalTech about using $\\LaTeX$ in Jupyter Notebook.](http://chebe163.caltech.edu/2018w/handouts/intro_to_latex.html). You could also find various $\\LaTeX$ tutorials and cheat sheets online.\n",
    "\n",
    "#### Include Images\n",
    "If you are still getting familiar with using LaTeX, handwrite the response on paper or the stylus. Take a picture or screenshot of your answer, and include that image in the Jupyter Notebook. Be sure to include that image in the `\\imgs` directory. Let's say you have your Q1 response saved as `imgs/Q1.png`; the markdown syntax to include that image is `![Q1](imgs/Q1.png)`. \n",
    "\n",
    "## Important Notice\n",
    "\n",
    "You must check both submission output on the gradescope (`Assignment 3` and `Assignment 3 - Manual Grading`) correctly reflects your work and responses. If you notice inconsistencies between your notebook and the Manual Grading portion, you need to make a campuswire post, and we can help you with that.\n",
    "\n",
    "**Other**\n",
    "\n",
    "If you are not feeling comfortable with the programming assignments in this homework, it might help to take a look at [https://github.com/UCSD-COGS108/Tutorials](https://github.com/UCSD-COGS108/Tutorials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a44005",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Run me\n",
    "#!pip uninstall -y otter-grader\n",
    "#!pip install -q  git+https://github.com/scott-yj-yang/otter-grader.git --no-warn-script-location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cba8e9",
   "metadata": {},
   "source": [
    "# Question 1: Conceptual Questions\n",
    "\n",
    "Multiple Choice Section\n",
    "\n",
    "Write your solution as a list of strings by replacing the \"...\" \n",
    "\n",
    "Ex.: `[\"A\", \"C\"]` if you think the answers are A and C). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb65f8c",
   "metadata": {},
   "source": [
    "## 1. Linear regression\n",
    "\n",
    "Choose <b>all</b> the valid answers to the description about <b>linear regression</b> and <b>logistic regression</b> from the options below:\n",
    "\n",
    "A. A linear regression model is linear (i.e., the features are combined linearly), while a logistic regression model is non-linear because its decision boundary is not a straight line.\n",
    "\n",
    "B. Both linear regression and logistic regression involve minimizing some 'loss'.\n",
    "\n",
    "C. Linear regression can only be solved by using closed-form solution while logistic regression does not have a closed-form solution.\n",
    "\n",
    "D. The output of a linear regression predictor can be any real value, while a logistic regression predictor can only give binary outputs (i.e. either -1 or +1) and it can never estimate the probability of an instance being a positive or negative case. \n",
    "\n",
    "_Points:_ 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb8b2202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q1_1 = [\"B\",\"C\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d634e624",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>MCQ_LR_1</pre></strong> passed! üåà</p>"
      ],
      "text/plain": [
       "MCQ_LR_1 results: All test cases passed!"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"MCQ_LR_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bf8135",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent\n",
    "\n",
    "Choose <b>all</b> the valid answers to the description about <b>gradient descent</b> from the options below:\n",
    "\n",
    "A. The general gradient descent method does not guarantee that the global minimum can always be reached, but there exists an algorithm that can always find the suitable learning-rate/step-size so that the global minimum can be reached after finite number of iterations.\n",
    "\n",
    "B. Loss is always decreased as long as we are moving in the opposite direction of its gradient at each step, while the step-size does not matter.\n",
    "\n",
    "C. With same initial weights and same training dataset, the algorithm should either reach the global minumum or end up at the same local minimum; the order in which data is fed does not matter.\n",
    "\n",
    "D. When the learning rate is high, though training may be faster, learning may 'jump' back and forth over the minima, thus make it even more difficult to find the optimal solution.\n",
    "\n",
    "_Points:_ 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11caa548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q1_2 = [\"A\", \"D\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76ad059e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>MCQ_LR_2</pre></strong> passed! üçÄ</p>"
      ],
      "text/plain": [
       "MCQ_LR_2 results: All test cases passed!"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"MCQ_LR_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f0901e",
   "metadata": {},
   "source": [
    "## 3. Classification\n",
    "\n",
    "Choose the <b>most significant</b> difference between regression and classification:\n",
    "\n",
    "A. Classification can handle categorical variables directly but regression can‚Äôt.\n",
    "\n",
    "B. Classification is supervised machine learning algorithm but regression is unsupervised machine learning algorithm.\n",
    "\n",
    "C. Classification's output is bounded (between -1 and 1) while regression can output value less than -1 or greater than 1.\n",
    "\n",
    "D. Classification aims to output discrete values (or categories) but regression aims to output continuous values (or numerical values).\n",
    "\n",
    "_Points:_ 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bdf488d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q1_3 = [\"D\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5839ed2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>MCQ_LR_3</pre></strong> passed! üöÄ</p>"
      ],
      "text/plain": [
       "MCQ_LR_3 results: All test cases passed!"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"MCQ_LR_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea6e74e",
   "metadata": {},
   "source": [
    "## 4. Classification (example)\n",
    "\n",
    "Assume we have a binary classification model:\n",
    "\n",
    "\\begin{align*}\n",
    "f(\\mathbf{x})&=\n",
    "\\begin{cases}\n",
    " 1, & w \\cdot x + b \\ge 0, \\\\\n",
    "-1, & w \\cdot x + b < 0. \n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "where there‚Äôs a feature vector $x = (x_1, x_2) \\in R^2$, bias $b \\in R$, and a weight vector $w = (w_1, w_2) \\in R^2$. The decision boundary of the classification model is: \n",
    "\n",
    "\\begin{align*}\n",
    " w \\cdot x + b = 0\n",
    "\\end{align*}\n",
    "\n",
    "Part 1)  If the predictions of the classifier f and its decision boundary $w \\cdot x + b = 0$ are shown in Figure 1, which one below can be a possible solution of weight vector $w$ and bias $b$? \n",
    "\n",
    "A. $w = (+1, +1); b = 0$\n",
    "\n",
    "B. $w = (+1, -1); b = 0$\n",
    "\n",
    "C. $w = (-1, -1); b = 0$\n",
    "\n",
    "D. $w = (-1, +1); b = 0$\n",
    "\n",
    "![Q1_reg_1](imgs/Q1_reg_1.png)\n",
    "\n",
    "_Points:_ 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60f636a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q1_4_1 = [\"A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cc9fb4c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>MCQ_LR_4_1</pre></strong> passed! üåà</p>"
      ],
      "text/plain": [
       "MCQ_LR_4_1 results: All test cases passed!"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"MCQ_LR_4_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1addcc9",
   "metadata": {},
   "source": [
    "Part 2)  If the predictions of the classifier f and its decision boundary $w \\cdot x + b = 0$ are shown in Figure 2, which one below can be a possible solution of weight vector $w$ and bias $b$? \n",
    "\n",
    "A. $w = (-1, +1); b = +1$\n",
    "\n",
    "B. $w = (-1, +1); b = -1$\n",
    "\n",
    "C. $w = (+1, -1); b = +1$\n",
    "\n",
    "D. $w = (+1, -1); b = -1$\n",
    "\n",
    "![Q1_reg_2](imgs/Q1_reg_2.png)\n",
    "\n",
    "_Points:_ 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5655c529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q1_4_2 = [\"D\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf2e429e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>MCQ_LR_4_2</pre></strong> passed! ‚ú®</p>"
      ],
      "text/plain": [
       "MCQ_LR_4_2 results: All test cases passed!"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"MCQ_LR_4_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7cc74",
   "metadata": {},
   "source": [
    "\n",
    "# Question 2: Logistic Regression Inference\n",
    "\n",
    "We have a logistic regression classifier for a 2-dimensional feature vector $\\mathbf{x}=(x_1,x_2)$:\n",
    "\\begin{align*}\n",
    "f(\\mathbf{x})&=\n",
    "\\begin{cases}\n",
    " 1, & \\frac{1}{1+e^{-(3x_1-4x_2+5)}} \\geq 0.5, \\\\\n",
    "-1, & \\text{otherwise}. \n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "where $f(\\mathbf{x})\\in \\{-1,1\\}$ is the predicted label. Please solve the following problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67a922c",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 2.1\n",
    "Draw the decision boundary of the logistic regression classifier and shade the region where the classifier predicts 1. Make sure you have marked the $x_1$ and $x_2$ axes and the intercepts on those axes.\n",
    "\n",
    "_Points:_ 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54ff2d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# the x1 and x2 of points on the decision boundary\n",
    "boundary_x1s = np.linspace(-2, 2, 20)\n",
    "# STEP 1\n",
    "boundary_x2s = np.linspace(-0.25, 11/4, 20)\n",
    "\n",
    "# plot the boundary\n",
    "plt.plot(boundary_x1s, boundary_x2s)\n",
    "plt.xlim([-2.2, 2.2])\n",
    "plt.ylim([-2.2, 2.2])\n",
    "plt.annotate('x1', [2, 0.2], weight='bold')\n",
    "plt.annotate('x2', [0.2, 2], weight='bold')\n",
    "\n",
    "# generate some random data points\n",
    "np.random.seed(0)\n",
    "N_samples = 60\n",
    "sample_x1s = np.random.rand(N_samples) * 4 - 2 \n",
    "sample_x2s = np.random.rand(N_samples) * 4 - 2\n",
    "\n",
    "# predict the labels (1 or -1) of the samples\n",
    "# STEP 2\n",
    "logit = np.exp(-(3*sample_x1s-4*sample_x2s+5))\n",
    "# STEP 3\n",
    "probabilities = 1/(1+logit)\n",
    "positive = probabilities >= 0.5\n",
    "negative = probabilities < 0.5\n",
    "\n",
    "# plot the samples\n",
    "plt.scatter(sample_x1s[positive], sample_x2s[positive], color=\"g\", marker=\"+\", label='positive')\n",
    "plt.scatter(sample_x1s[negative], sample_x2s[negative], color=\"r\", marker=\"_\", label='negative')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.set_xticks( [-2, -1, 0, 1, 2])\n",
    "ax.set_yticks( [-2, -1, 1, 2])\n",
    "ax.set_aspect(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a63e882",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 2.2\n",
    "\n",
    "Solve this problem using handwritten / LaTeX math.\n",
    "For the new data point $\\mathbf{x}_1=(3,4)$:\n",
    "\n",
    "(1) Predict its label;\n",
    "\n",
    "(2) Give the estimated probability of it being a <b>negative</b> sample (round your answer to 0.01);\n",
    "\n",
    "(3) Compute its distance to the decision boundary.\n",
    "\n",
    "_Points:_ 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58b78f9",
   "metadata": {},
   "source": [
    "1. $$ \\begin{aligned} e^{(-(3x_1-4x_2+5))} &= \\\\ e^{(-(3(3)-4(4)+5))} &= \\\\ e^{2} &= 7.389 \\end{aligned}$$\n",
    "<br />\n",
    "\n",
    "2. $$ \\begin{aligned} 1/(1+7.389) < 0.5 \\\\  1/8.389 < 0.5 \\\\ 0.2384 \\approx 0.24 \\end{aligned} $$ \n",
    "<br />\n",
    "\n",
    "3. $$ \\begin{aligned} w &= \\begin{bmatrix} - \\frac{1}{\\sqrt{0.75^2 + 1^2}} \\\\ \\frac{1}{\\sqrt{0.75^2 + 1^2}} \\end{bmatrix} \\\\ w &= \\begin{bmatrix} - \\frac{1}{\\sqrt{1.5625}} \\\\ \\frac{1}{\\sqrt{1.5625}} \\end{bmatrix} \\\\ w^T &= \\begin{bmatrix} - \\frac{1}{\\sqrt{1.5625}} \\frac{1}{\\sqrt{1.5625}} \\end{bmatrix} \\\\\n",
    "w^Tx &= \\begin{bmatrix} - \\frac{1}{\\sqrt{1.5625}} \\frac{1}{\\sqrt{1.5625}} \\end{bmatrix} * \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} \\\\ w^Tx &= 0.8\n",
    "\\end{aligned}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac6514",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Question 3. Logistic Regression\n",
    "\n",
    "Assume in a binary classification problem, we need to predict a binary label $y\\in \\{-1,1\\}$ for a feature vector $x=[x_0,x_1]^\\top$. In logistic regression, we can reformulate the binary classification problem in a probabilistic framework: We aim to model the distribution of classes given the input feature vector $x$. Specifically, we can express the conditional probability $p(y|x)$ parameterized by $(\\mathbf w, b)$ using logistic function as:\n",
    "\\begin{align*}\n",
    "\\label{eq:logistic-regression-model}\n",
    "p(y|x)=\\frac{1}{1+e^{-y(\\mathbf w \\cdot x + b)}}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{w}=[w_0,w_1]^\\top$ is the weight vector, and $b$ is the bias scalar. Given a training dataset $S_\\text{training} = \\{(x_i, y_i)\\}, i=1,\\ldots,n\\}$, we wish to optimize the negative log-likelihood loss $\\mathcal{L}(\\mathbf w, b)$:\n",
    "\n",
    "$$ \\mathcal{L}(\\mathbf w, b)=-\\sum_{i=1}^{n} \\ln p(y_i|x_i) $$\n",
    "\n",
    "and find the optimal weight vector $\\mathbf{w}$ and bias $b$ to build the logistic regression model:\n",
    "$$ \\mathbf{w}^*, b^* = \\arg\\min_{\\mathbf{w}, b}\\mathcal{L}(\\mathbf{w}, b) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8611a39a",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 3.1\n",
    "In this problem, we attempt to obtain the optimal parameters $\\mathbf{w}^*$ and $b^*$ by using a standard gradient descent algorithm. Assume $p_i = p(y_i|x_i)$, solve for the gradient for $\\mathbf w$ and the gradient for b.\n",
    "\n",
    "_Points:_ 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2dfcbf",
   "metadata": {},
   "source": [
    "$$ \\begin{aligned} \\Delta \\mathbf{w} &= \\sum_{i=1}^{n} -y_i x_i (1 - p_i) \\\\ \\Delta b &= \\sum_{i=1}^{n} -y_i (1 - p_i) \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f19532",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 3.2\n",
    "In reality, we typically tackle this problem in a matrix form: First, we represent data points as matrices $X=[\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n]^T$ and $Y=[y_1, y_2, \\ldots, y_n]^T$. Thus, the negative log-likelihood loss $\\mathcal{L}(\\mathbf{w},b)$ can be formulated as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{p} = \\text{sigmoid}(Y \\circ (X\\mathbf{w} + b\\mathbf{1})),  \\quad\\quad \\mathcal{L}(\\mathbf w, b)=-\\mathbf{1}^T \\ln \\mathbf{p}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{1} = [1,1,...,1]^T \\in \\mathbb{R}^n$ is a $n$-dimensional column vector, $\\mathbf{p} = [p_1,p_2,...,p_n]^T \\in \\mathbb{R}^n$ is a $n$-dimensional column vector, $\\ln(\\cdot)$ is an element-wise natural logarithm function, $\\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}}$ is an element-wise sigmoid function, and $\\circ$ is an element-wise product operator. Use this new information to reformulate the gradients for w and b in matrix form.\n",
    "\n",
    "_Points:_ 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140f2363",
   "metadata": {},
   "source": [
    "$$ \\begin{aligned} \\Delta \\mathbf{w} &= -X^T (Y*(1 - p_i)) \\\\ \\Delta b &= -1^T (Y*(1 - p_i)) \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e023f2e",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Question 4. Decision Tree\n",
    "\n",
    "As we recall, the underlying uncertainty of a variable is measured by entropy\n",
    "\n",
    "\\begin{align*}\n",
    "H(X) = -\\sum_iP(X=x_i)\\log_2(P(X=x_i))\n",
    "\\end{align*}\n",
    "\n",
    "When building a tree, one way to decide how to group the values is with the following formula:\n",
    "\n",
    "\\begin{align*}\n",
    "\\arg \\max _B G(S, B)=H(S)-\\sum_{i=1}^t \\frac{\\left|S_i\\right|}{|S|} H\\left(S_i\\right)\n",
    "\\end{align*}\n",
    "\n",
    "where S is the state of the variable of interest, B represents the split that we choose. The larger the value we get the better fit that tree that we made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2261520",
   "metadata": {},
   "source": [
    "You are conducting an fMRI study and discover that when the subject is performing a certain task, some areas in brain are more activated while others are more inhibited. For this experiment, voxels sampled from the region of interest are labeled as either 'excitatory' (green circle) or 'inhibitory' (red triangle). Now you wonder whether we can predict task-induced activation based on certain features of a region, and you hypothesize that feature A and feature B are the most relevant. \n",
    "\n",
    "![Q4_DT](imgs/Q4_DT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945af075",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 4.1 \n",
    "Use the formula to calculate the entropy of activation (i.e. excitation or inhibition) in the region of interest. Show your work. Round your answer to 3 decimal places (0.001).\n",
    "\n",
    "_Points:_ 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f4a9e5",
   "metadata": {},
   "source": [
    "$$ \\begin{aligned}\n",
    "H(X) &= -\\sum_{i} P(X=x_i)\\log_2(P(X=x_i)) \\\\\n",
    "H(X) &= - ((\\frac{4}{9}) * \\log_2(\\frac{4}{9}) +  (\\frac{5}{9}) * \\log_2(\\frac{5}{9})) \\\\\n",
    "H(X) &= - (-0.51996 + -0.47110) \\\\ \n",
    "H(X) &= 0.991\n",
    "\\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f2cd7",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 4.2\n",
    "Now we want to determine whether a region is excitatory or inhibitory based on feature A, i.e. a region is predicted excitatory/inhibitory when its feature A is above certain value. What will be the best split? Give your answer as 'The split should be at feature A = ...(1/2/3/4)', and compute the gain. Round your answer to 3 decimal places (0.001).\n",
    "\n",
    "_Points:_ 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c63e3b0",
   "metadata": {},
   "source": [
    "The split should be at feature A = 2\n",
    "\n",
    "$$ G(S,B) = H(S) - \\sum_{i=1}^t \\frac{\\left|S_i\\right|}{|S|} H\\left(S_i\\right)$$\n",
    "When split is A = 2 ...\n",
    "$$ \\begin{aligned} \n",
    "G(S,B) &= H(S) - (G(S_{left}) + G(S_{right})) \\\\\n",
    "G(S,B) &= H(S) - ((-\\frac{5}{9}((\\frac{1}{5}\\log_2(\\frac{1}{5})) + (\\frac{4}{5}\\log_2(\\frac{4}{5})))) + (-\\frac{4}{9}((\\frac{3}{4}\\log_2(\\frac{3}{4})) + (\\frac{1}{4}\\log_2(\\frac{1}{4}))))) \\\\\n",
    "G(S,B) &= H(S) - 0.76163921 \\\\ \n",
    "G(S,B) &= 0.991 - 0.7616 \\\\\n",
    "G(S,B) &= 0.229\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e923186",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 4.3\n",
    "Similarly, what will be the best split based on feature B? Give your answer as 'The split should be at feature B = ...(1/2)', and compute the gain. Round your answer to 3 decimal places (0.001).\n",
    "\n",
    "_Points:_ 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b47e35",
   "metadata": {},
   "source": [
    "The split should be at feature B = 2\n",
    "\n",
    "$$ G(S,B) = H(S) - \\sum_{i=1}^t \\frac{\\left|S_i\\right|}{|S|} H\\left(S_i\\right)$$\n",
    "When split is B = 2 ...\n",
    "$$ \\begin{aligned} \n",
    "G(S,B) &= H(S) - (G(S_{bottom}) + G(S_{top})) \\\\\n",
    "G(S,B) &= H(S) - ((-\\frac{5}{9}((\\frac{3}{5}\\log_2(\\frac{3}{5})) + (\\frac{2}{5}\\log_2(\\frac{2}{5})))) + (-\\frac{4}{9}((\\frac{1}{4}\\log_2(\\frac{1}{4})) + (\\frac{3}{4}\\log_2(\\frac{3}{4}))))) \\\\\n",
    "G(S,B) &= H(S) - 0.8999 \\\\ \n",
    "G(S,B) &= 0.991 - 0.8999 \\\\\n",
    "G(S,B) &= 0.091\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a37984",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 4.4\n",
    "When constructing a decision tree classifier, what are some good and bad rules of thumb and why?\n",
    "\n",
    "_Points:_ 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db7237",
   "metadata": {},
   "source": [
    "Some good rules of thumb include trying to create the lowest entropy and spreading out the tree evenly, which allows for less disorder and quicker prediction thumbs. Thus this means we also are wanting to create a shallow tree instead of a deep tree to keep the complexity level low, as well as trying to create a balanced tree to minimize overfitting. This might look like minimizing the recursive funciton. On the other hand, for bad rules of thumb, we should be avoiding the opposite. Thus, we should avoid creating deep and unbalanced trees to avoid overfitting and high complexity levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c243c",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# End of A3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ee059",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit.\n",
    "\n",
    "Please make sure to see the output of the gradescope autograder. You are responsible for waiting and ensuring that the autograder is executing normally for your submission. Please create a campuswire post if you see errors in autograder execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2368115e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.export(pdf=False, force_save=True, run_tests=True, files=['imgs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c7490",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "MCQ_LR_1": {
     "name": "MCQ_LR_1",
     "points": 0.2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # sanity check\n>>> all_options = [\"A\", \"B\", \"C\", \"D\"]\n>>> check_valid = lambda ans, all_options: all([chosen in all_options for chosen in ans])\n>>> assert check_valid(q1_1, all_options), \"Is your answer within the option of A/B/C/D?\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "MCQ_LR_2": {
     "name": "MCQ_LR_2",
     "points": 0.2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # sanity check\n>>> all_options = [\"A\", \"B\", \"C\", \"D\"]\n>>> check_valid = lambda ans, all_options: all([chosen in all_options for chosen in ans])\n>>> assert check_valid(q1_2, all_options), \"Is your answer within the option of A/B/C/D?\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "MCQ_LR_3": {
     "name": "MCQ_LR_3",
     "points": 0.2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # sanity check\n>>> all_options = [\"A\", \"B\", \"C\", \"D\"]\n>>> check_valid = lambda ans, all_options: all([chosen in all_options for chosen in ans])\n>>> assert check_valid(q1_3, all_options), \"Is your answer within the option of A/B/C/D?\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "MCQ_LR_4_1": {
     "name": "MCQ_LR_4_1",
     "points": 0.2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # sanity check\n>>> all_options = [\"A\", \"B\", \"C\", \"D\"]\n>>> check_valid = lambda ans, all_options: all([chosen in all_options for chosen in ans])\n>>> assert check_valid(q1_4_1, all_options), \"Is your answer within the option of A/B/C/D?\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "MCQ_LR_4_2": {
     "name": "MCQ_LR_4_2",
     "points": 0.2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # sanity check\n>>> all_options = [\"A\", \"B\", \"C\", \"D\"]\n>>> check_valid = lambda ans, all_options: all([chosen in all_options for chosen in ans])\n>>> assert check_valid(q1_4_2, all_options), \"Is your answer within the option of A/B/C/D?\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
